#include <vector>
#include <common/maca_fp16.h>
#include <algorithm>
#include <cmath>

#include "../tester/utils.h"

// ============================================================================
// 平台适配配置 - 沐曦 MACA
// ============================================================================

#define PLATFORM_NAME "MetaX MACA"

// 通用配置
#define WARP_SIZE 32
#define MAX_THREADS_PER_BLOCK 1024
#define TRACE_BLOCK_SIZE 256
#define TRACE_CPU_THRESHOLD 64

// ============================================================================
// TRACE FUNCTION IMPLEMENTATION (自适应优化版本)
// ============================================================================

/**
 * @brief MACA kernel to compute partial trace sums per block
 */
template <typename T>
__global__ void traceKernel(const T* input, size_t rows, size_t cols, T* block_results) {
    extern __shared__ char shared_mem[];
    T* sdata = reinterpret_cast<T*>(shared_mem);
    
    int tid = threadIdx.x;
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    size_t diag_size = min(rows, cols);
    
    // 加载对角线元素
    T value = T(0);
    if (idx < diag_size) {
        value = input[idx * cols + idx];
    }
    sdata[tid] = value;
    __syncthreads();
    
    // Block 内树状归约
    for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] += sdata[tid + s];
        }
        __syncthreads();
    }
    
    if (tid == 0) {
        block_results[blockIdx.x] = sdata[0];
    }
}

/**
 * @brief Second-stage reduction kernel for large matrices
 */
template <typename T>
__global__ void reduceBlockResults(const T* block_results, int num_blocks, T* output) {
    extern __shared__ char shared_mem[];
    T* sdata = reinterpret_cast<T*>(shared_mem);
    
    int tid = threadIdx.x;
    
    // 每个线程加载多个元素
    T sum = T(0);
    for (int i = tid; i < num_blocks; i += blockDim.x) {
        sum += block_results[i];
    }
    sdata[tid] = sum;
    __syncthreads();
    
    // 树状归约
    for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] += sdata[tid + s];
        }
        __syncthreads();
    }
    
    if (tid == 0) {
        *output = sdata[0];
    }
}

/**
 * @brief Computes the trace of a matrix.
 *
 * The trace of a matrix is defined as the sum of its diagonal elements.
 * This function expects a flattened row-major matrix stored in a
 * std::vector. If the matrix is not square, the trace will sum up
 * elements along the main diagonal up to the smaller of rows or cols.
 *
 * Strategy selection based on matrix size:
 * - Very small (diag <= 256): CPU direct computation
 * - Small (1 block): Single GPU kernel
 * - Medium (2-64 blocks): CPU reduction (fast for small data)
 * - Large (>64 blocks): Two-stage GPU reduction
 *
 * Platform: MetaX MACA
 *
 * @tparam T The numeric type of matrix elements (e.g., float, int).
 * @param h_input A flattened matrix of size rows * cols.
 * @param rows Number of rows in the matrix.
 * @param cols Number of columns in the matrix.
 * @return The trace (sum of diagonal values) of the matrix.
 */
template <typename T>
T trace(const std::vector<T>& h_input, size_t rows, size_t cols) {
    if (h_input.empty() || rows == 0 || cols == 0) {
        return T(0);
    }
    
    size_t diag_size = std::min(rows, cols);
    size_t input_size = rows * cols;
    
    // ========================================================================
    // Strategy 1: Very small matrices - CPU direct computation
    // ========================================================================
    if (diag_size <= 256) {
        T result = T(0);
        for (size_t i = 0; i < diag_size; i++) {
            result += h_input[i * cols + i];
        }
        return result;
    }
    
    int blockSize = 256;
    int numBlocks = (diag_size + blockSize - 1) / blockSize;
    
    T* d_input = nullptr;
    T* d_block_results = nullptr;
    
    macaMalloc(&d_input, input_size * sizeof(T));
    macaMalloc(&d_block_results, numBlocks * sizeof(T));
    macaMemcpy(d_input, h_input.data(), input_size * sizeof(T), macaMemcpyHostToDevice);
    
    size_t sharedMemSize = blockSize * sizeof(T);
    
    // Phase 1: GPU block-level reduction (99%+ of computation)
    traceKernel<<<numBlocks, blockSize, sharedMemSize>>>(
        d_input, rows, cols, d_block_results);
    
    T result;
    
    // ========================================================================
    // Strategy 2: Single block - direct copy
    // ========================================================================
    if (numBlocks == 1) {
        macaMemcpy(&result, d_block_results, sizeof(T), macaMemcpyDeviceToHost);
    }
    // ========================================================================
    // Strategy 3: Small number of blocks (2-64) - CPU reduction
    // Rationale: For <64 values, CPU reduction is faster than kernel launch
    // ========================================================================
    else if (numBlocks <= 64) {
        std::vector<T> h_block_results(numBlocks);
        macaMemcpy(h_block_results.data(), d_block_results, 
                   numBlocks * sizeof(T), macaMemcpyDeviceToHost);
        
        result = T(0);
        // Manual unrolling for better performance with -O0
        int i = 0;
        for (; i + 4 <= numBlocks; i += 4) {
            result += h_block_results[i] + h_block_results[i+1] + 
                      h_block_results[i+2] + h_block_results[i+3];
        }
        for (; i < numBlocks; i++) {
            result += h_block_results[i];
        }
    }
    // ========================================================================
    // Strategy 4: Large number of blocks (>64) - GPU second-stage reduction
    // Rationale: Kernel launch overhead < CPU reduction time for many blocks
    // ========================================================================
    else {
        T* d_output = nullptr;
        macaMalloc(&d_output, sizeof(T));
        
        int reduceBlockSize = 256;
        reduceBlockResults<<<1, reduceBlockSize, reduceBlockSize * sizeof(T)>>>(
            d_block_results, numBlocks, d_output);
        
        macaMemcpy(&result, d_output, sizeof(T), macaMemcpyDeviceToHost);
        macaFree(d_output);
    }
    
    macaFree(d_input);
    macaFree(d_block_results);
    
    return result;
}


// ============================================================================
// FLASH ATTENTION IMPLEMENTATION
// ============================================================================

/**
 * @brief Flash Attention kernel with numerical stability for MACA
 * 
 * Features:
 * - Grouped Query Attention (GQA) support
 * - Causal masking for autoregressive models
 * - Numerically stable softmax (max subtraction)
 * - Shared memory optimization
 * 
 * Platform: MetaX MACA
 */
template <typename T>
__global__ void flashAttentionKernel(
    const T* Q, const T* K, const T* V, T* O,
    int batch_size, int tgt_seq_len, int src_seq_len,
    int query_heads, int kv_heads, int head_dim,
    bool is_causal
) {
    int global_idx = blockIdx.x;
    int total_queries = batch_size * tgt_seq_len * query_heads;
    
    if (global_idx >= total_queries) return;
    
    // 解码索引
    int batch_idx = global_idx / (tgt_seq_len * query_heads);
    int remaining = global_idx % (tgt_seq_len * query_heads);
    int query_pos = remaining / query_heads;
    int query_head = remaining % query_heads;
    
    // GQA mapping
    int kv_head = query_head * kv_heads / query_heads;
    
    int tid = threadIdx.x;
    int num_threads = blockDim.x;
    
    // Shared memory layout
    extern __shared__ float smem[];
    float* s_scores = smem;
    float* s_reduction = smem + src_seq_len;
    
    float scale = rsqrtf(float(head_dim));
    
    // Step 1: Compute attention scores
    for (int k_pos = tid; k_pos < src_seq_len; k_pos += num_threads) {
        float score;
        
        if (is_causal && k_pos > query_pos) {
            score = -1e9f;
        } else {
            score = 0.0f;
            for (int d = 0; d < head_dim; d++) {
                int q_idx = ((batch_idx * tgt_seq_len + query_pos) * query_heads + query_head) * head_dim + d;
                int k_idx = ((batch_idx * src_seq_len + k_pos) * kv_heads + kv_head) * head_dim + d;
                score += float(Q[q_idx]) * float(K[k_idx]);
            }
            score *= scale;
        }
        
        s_scores[k_pos] = score;
    }
    __syncthreads();
    
    // Step 2: Find max for numerical stability
    float local_max = -1e9f;
    for (int k_pos = tid; k_pos < src_seq_len; k_pos += num_threads) {
        local_max = fmaxf(local_max, s_scores[k_pos]);
    }
    
    s_reduction[tid] = local_max;
    __syncthreads();
    
    for (unsigned int s = num_threads / 2; s > 0; s >>= 1) {
        if (tid < s) {
            s_reduction[tid] = fmaxf(s_reduction[tid], s_reduction[tid + s]);
        }
        __syncthreads();
    }
    float max_score = s_reduction[0];
    __syncthreads();
    
    // Step 3: Compute exp and sum
    float local_sum = 0.0f;
    for (int k_pos = tid; k_pos < src_seq_len; k_pos += num_threads) {
        float exp_val = expf(s_scores[k_pos] - max_score);
        s_scores[k_pos] = exp_val;
        local_sum += exp_val;
    }
    
    s_reduction[tid] = local_sum;
    __syncthreads();
    
    for (unsigned int s = num_threads / 2; s > 0; s >>= 1) {
        if (tid < s) {
            s_reduction[tid] += s_reduction[tid + s];
        }
        __syncthreads();
    }
    float sum_exp = s_reduction[0];
    __syncthreads();
    
    // Step 4: Normalize
    float inv_sum = 1.0f / fmaxf(sum_exp, 1e-10f);
    for (int k_pos = tid; k_pos < src_seq_len; k_pos += num_threads) {
        s_scores[k_pos] *= inv_sum;
    }
    __syncthreads();
    
    // Step 5: Compute output
    for (int d = tid; d < head_dim; d += num_threads) {
        float out_val = 0.0f;
        for (int k_pos = 0; k_pos < src_seq_len; k_pos++) {
            int v_idx = ((batch_idx * src_seq_len + k_pos) * kv_heads + kv_head) * head_dim + d;
            out_val += s_scores[k_pos] * float(V[v_idx]);
        }
        
        int o_idx = ((batch_idx * tgt_seq_len + query_pos) * query_heads + query_head) * head_dim + d;
        O[o_idx] = T(out_val);
    }
}

/**
 * @brief Computes flash attention for given query, key, and value tensors.
 * 
 * Implements efficient attention mechanism with support for:
 * - Multi-head attention
 * - Grouped query attention (GQA)
 * - Causal masking
 * 
 * Platform: MetaX MACA
 * 
 * @tparam T Data type (float) for input/output tensors
 * @param[in] h_q Query tensor of shape [batch_size, tgt_seq_len, query_heads, head_dim]
 * @param[in] h_k Key tensor of shape [batch_size, src_seq_len, kv_heads, head_dim]
 * @param[in] h_v Value tensor of shape [batch_size, src_seq_len, kv_heads, head_dim]
 * @param[out] h_o Output attention tensor of shape [batch_size, tgt_seq_len, query_heads, head_dim]
 * @param[in] batch_size Batch dimension size
 * @param[in] target_seq_len Target sequence length
 * @param[in] src_seq_len Source sequence length  
 * @param[in] query_heads Number of query attention heads
 * @param[in] kv_heads Number of key/value heads (supports grouped query attention)
 * @param[in] head_dim Dimension size of each attention head
 * @param[in] is_causal Whether to apply causal masking
 */
template <typename T>
void flashAttention(const std::vector<T>& h_q, const std::vector<T>& h_k,
                    const std::vector<T>& h_v, std::vector<T>& h_o,
                    int batch_size, int target_seq_len, int src_seq_len, 
                    int query_heads, int kv_heads, int head_dim, bool is_causal) {
    
    if (batch_size == 0 || target_seq_len == 0 || src_seq_len == 0 || 
        query_heads == 0 || kv_heads == 0 || head_dim == 0) {
        h_o.resize(batch_size * target_seq_len * query_heads * head_dim, T(0));
        return;
    }
    
    size_t q_size = batch_size * target_seq_len * query_heads * head_dim;
    size_t k_size = batch_size * src_seq_len * kv_heads * head_dim;
    size_t v_size = batch_size * src_seq_len * kv_heads * head_dim;
    size_t o_size = batch_size * target_seq_len * query_heads * head_dim;
    
    T *d_q, *d_k, *d_v, *d_o;
    macaMalloc(&d_q, q_size * sizeof(T));
    macaMalloc(&d_k, k_size * sizeof(T));
    macaMalloc(&d_v, v_size * sizeof(T));
    macaMalloc(&d_o, o_size * sizeof(T));
    
    macaMemcpy(d_q, h_q.data(), q_size * sizeof(T), macaMemcpyHostToDevice);
    macaMemcpy(d_k, h_k.data(), k_size * sizeof(T), macaMemcpyHostToDevice);
    macaMemcpy(d_v, h_v.data(), v_size * sizeof(T), macaMemcpyHostToDevice);
    
    int total_queries = batch_size * target_seq_len * query_heads;
    int threads = 256;
    int blocks = total_queries;
    
    size_t smem_size = (src_seq_len + threads) * sizeof(float);
    
    flashAttentionKernel<<<blocks, threads, smem_size>>>(
        d_q, d_k, d_v, d_o,
        batch_size, target_seq_len, src_seq_len,
        query_heads, kv_heads, head_dim, is_causal
    );
    
    macaError_t err = macaDeviceSynchronize();
    if (err != macaSuccess) {
        printf("[%s] MACA Error: %s\n", PLATFORM_NAME, macaGetErrorString(err));
    }
    
    h_o.resize(o_size);
    macaMemcpy(h_o.data(), d_o, o_size * sizeof(T), macaMemcpyDeviceToHost);
    
    macaFree(d_q);
    macaFree(d_k);
    macaFree(d_v);
    macaFree(d_o);
}


// *********************************************************************
// Explicit Template Instantiations (REQUIRED FOR LINKING WITH TESTER.O)
// DO NOT MODIFY THIS SECTION
// *********************************************************************
template int trace<int>(const std::vector<int>&, size_t, size_t);
template float trace<float>(const std::vector<float>&, size_t, size_t);
template void flashAttention<float>(const std::vector<float>&, const std::vector<float>&,
  const std::vector<float>&, std::vector<float>&,
  int, int, int, int, int, int, bool);
template void flashAttention<half>(const std::vector<half>&, const std::vector<half>&,
  const std::vector<half>&, std::vector<half>&,
  int, int, int, int, int, int, bool);
